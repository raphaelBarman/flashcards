% -*- coding-system:utf-8 
% LATEX PREAMBLE --- needs to be imported manually
\documentclass[12pt]{article}
\special{papersize=3in,5in}
\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsmath}
\pagestyle{empty}
\setlength{\parindent}{0in}

%%% commands that do not need to imported into Anki:
\usepackage{mdframed}
\newcommand*{\xfield}[1]{\begin{mdframed}\centering #1\end{mdframed}\bigskip}
\newenvironment{note}{}{}
\newcommand*{\tags}[1]{\paragraph{tags: }#1} 
% END OF THE PREAMBLE
\begin{document}

\begin{note}
	\tags{}
	\xfield{
		Donner la formule de l'entropie.	
	}
	\xfield{
		$$H(S) = \sum\limits_{s \in S} p(s)\log_2\left(\frac{1}{p(s)}\right) = -\sum\limits_{s \in S} p(s)\log_2(p(s))$$	
	}
\end{note}

\begin{note}
	\xfield{Si les $M$ symboles de la source $S$ sont équiprobables, que vaut $H(S)$ ?}
	\xfield{$H(S) = \log_2(M)$}
\end{note}

\begin{note}
	\xfield{Soit $S=(S_1,\hdots,S_n)$ une source composée, comment est bornée son entropie ?}
	\xfield{\begin{itemize}
	\item $H(S_1,\hdots,S_n) \le H(S_1) + \hdots + H(S_n)$
	\item $H(S_1,\hdots,S_n) = H(S_1) + \hdots + H(S_n)$ si et seulement si les $n$ sources marginales $S_1,\hdots,S_n$ sont indépendantes.
	\end{itemize}}
\end{note}

\begin{note}
	\xfield{Quel est le lien entre code sans préfixe et code instantané}
	\xfield{Un code est sans préfixe si et seulement si il est instantané}
\end{note}
\begin{note}
	\xfield{Donner l'inégalité de Kraft}
	\xfield{Soit $\Gamma$ un code D-aire dont les longueurs de mots $M$ sont $\ell_1,\hdots,\ell_M$.\begin{align*}
	\text{Décodage unique} \rightarrow D^{-\ell_1}+ \hdots + D^{-\ell_M} \le 1
	\end{align*}
	Par contraposée, si l'inégalité n'est pas respectée, le code n'est pas à décodage unique.\\
	Attention : si l'inégalité est respéctée, le code n'est pas forcément à décodage unique, mais il existe un code avec des mots de même longeur qui est à décodage unique.}
\end{note}
\begin{note}
	\xfield{Donner la formule de la longeur moyenne d'un code}
	\xfield{Soit une source $S$ d'alphabet $\mathcal{A}$ et de densité de probabilité $p$ et soit $\Gamma$ un code $D$-aire de la source $S$. Alors la longueur moyenne est :
	\begin{align*}
	L(\Gamma) = \sum\limits_{s \in \mathcal{A}} p(s)\ell(\Gamma(s))
	\end{align*}
	avec pour unité "le symbole de code par symbole de source"\\
	 (Si $D=2$ on dit "bits par symbole de source")}
\end{note}
\begin{note}
\xfield{Donner la première inégalité de l'entropie (elle est valable pour tout code à décodage unique)}
\xfield{Soit $\Gamma$ un code $D$-aire d'une source $S$. Si $\Gamma$ est à décodage unique sa longeur moyenne satisfait : \begin{align*}
L(\Gamma) \ge \frac{H(S)}{\log_2(D)}
\end{align*}}
\end{note}

\begin{note}
\xfield{Donner la deuxième inégalité de l'entropie (valable pour tout code de Shannon-Fano}
\xfield{$\frac{H(S)}{\log_2(D)} \le L(\Gamma_{SF}) < \frac{H(S)}{\log_2(D)} + 1$}
\end{note}

\begin{note}
	\xfield{Donner l'inégalité pour un code de Huffman}
	\xfield{$L(\Gamma_H) \le L(\Gamma)$}
\end{note}

\begin{note}
	\xfield{Soit $S=(S_1,S_2)$ une source composée. Quelle est l'entropie conditionnelle de $S_2$ sachant $S_1$ ?}
	\xfield{Tout d'abord, voyons comment calculer l'entropie de $S_2$ sachant que $S1=s_1$ : \begin{align*}
	H(S_2|S_1 = s_1) = - \sum\limits_{s_2 \in \mathcal{A}_2} p_{s_2|s_1}(s_2|s_1) \cdot \log_2(p_{s_2|s_1}(s_2|s_1))
	\end{align*}
	L'entropie conditionelle de $S_2$ sachant $S_1$ en est la moyenne :
	\begin{align*}
	H(S_2|S_1)= \sum\limits_{s_1 \in \mathcal{A}_1} H(S_2|S_1=s_1) \cdot p_{s_1}(s_1)
	\end{align*}}
\end{note}

\begin{note}
\xfield{Donner la règle de l'enchaînement utile pour le calcul de l'entropie conditionnelle}
\xfield{Soit $S=(S_1,S_2)$ une source composée. \begin{align*}
	H(S_1,S_2) &= H(S_1) + H(S_2|S_1)\\
	&= H(S_2) + H(S_1|S_2)
\end{align*}}
\end{note}

\begin{note}
	\xfield{Comment conditionner affecte l'entropie ? (pour une source $S_a=(S_{a1},S_{a2})$ et pour une source $S_b=(S_{b1},S_{b2},S_{b3}))$}
	\xfield{Conditionner réduit l'entropie :
	\begin{align*}
	&H(S_{a2} |S_{a1}) \le S_{a2}\\
	&H(S_{a2} | S_{a1}) = S_{a2} \text{ Si et seulement si $S_{a1}$ et $S_{a2}$ sont indépendantes}\\
	&H(S_{b3}|S_{b1},S_{b2}) \le H(S_{b3}|S_{b2})
	\end{align*}}
\end{note}
\begin{note}
\xfield{Donner le théorème du calcul incrémental de l'entropie conditionnelle pour une source $S=(S_1,S_2,\hdots,S_n)$}
\xfield{\begin{align*}
H(S_1,S_2,\hdots,S_n) &= H(S_n|H(S_1,S_2,\hdots,S_{n-1}) +  H(S_{n-1}|H(S_1,S_2,\hdots,S_{n-2})\\
&+ \hdots + H(S_3|H(S_1,S_2) + H(S_2|S_1) + H(S_1)
\end{align*}
}
\end{note}
\begin{note}
	\xfield{Expliquer ce que cela veut dire si une source est fonction d'une autre et qu'elle est la condition pour que cela soit le cas}
	\xfield{Une source est fonction d'une autre si connaître le premier implique le second.\\
	$S_2$ est fonction de $S_1$ si et seulement si $H(S_2|S_1) = 0$\\
	Aussi, si $S_2$ est fonction de $S_1$, alors :
	\begin{align*}
	H(S_1,S_2) &= H(S_1)\\
	H(S_2) &\le H(S_1)
	\end{align*}}
\end{note}

\begin{note}
	\xfield{Quels sont les deux conditions pour qu'une source $S$ soit une source étendue ?}
	\xfield{\begin{enumerate}
	\item $S^n$ est une source à $n$ composantes, sur l'alphabet $\underbrace{A\times \hdots \times A}_{n \text{ fois}}$; notons $p_{S^n}$ sa densité de probabilité.
	\item La densité de porbabilité de la source constituée des $n$ premières sources marginales de $S^{n+k}$ est égale à $p_{S^n}$, pour tous $k \ge 1$ et $n\ge 1$.
	\end{enumerate}}
\end{note}

\begin{note}
	\xfield{Quels sont les conditions pour qu'une source étendue soit \textbf{régulière} ?}
	\xfield{\begin{enumerate}
	\item $H(S) = \lim\limits_{n \to +\infty} H(S_n)$
	\item $H^*(S) = \lim\limits_{n \to +\infty} H(S_n|S_1,S_2,\hdots,S_{n-1})$
	\end{enumerate}}
\end{note}

\begin{note}
	\xfield{Pour une source étendue, comment calculer :
	\begin{enumerate}
	\item l'entropie d'\textbf{un} symbole
	\item l'entropie \textbf{par} symbole
	\item les \textbf{bits} par symbole 
	\end{enumerate} ?}
	\xfield{\begin{enumerate}
	\item $H(S) = \lim\limits_{n \to +\infty} H(S_n)$
	\item $H^*(S) = \lim\limits_{n \to +\infty} H(S_n|S_1,S_2,\hdots,S_{n-1})$
	\item $\frac{L^n}{n}$
	\end{enumerate} }
\end{note}

\begin{note}
	\xfield{Quel est l'entropie d'un bloc d'une source étendue régulière ?}
	\xfield{\begin{align*}
	&\lim\limits_{n \to +\infty} \frac{H(S_1,S_2,\hdots,S_n)}{n} = H^*(S)\\
	\text{On sait aussi que } &\lim\limits_{n \to +\infty} \frac{L^n_{SF}}{n}=\lim\limits_{n \to +\infty}\frac{L^n_H}{n}=\frac{H^*(S)}{\log_2(D)}
	\end{align*}}
\end{note}

\begin{note}
	\xfield{Quel est le rapport de compression d'un code d'une source étendue ?}
	\xfield{Il s'agit du rapport de sa longeur moyenne et celle d'une code "stupide". La formule exacte est :
	\begin{align*}
		\frac{L^n}{n\lceil\log_2(M)\rceil}
	\end{align*}
	Le rapport de compression d'un code optimal est 
	\begin{align*}
		\sim \frac{H^*(S)}{\log_2(D)}
	\end{align*}}
\end{note}

\begin{note}
	\xfield{Quel inégalité doit satisfaire les trois composants ($P=$\textit{Plaintext}, $C=$\textit{Cyphertext}, $K=$\textit{key}) pour être un système de cryptographie à confidentialité parfaite ?}
	\xfield{$H(P)\le H(C) \le H(K)$}
\end{note}

\begin{note}
	\tags{}
	\xfield{
		Donner l'inverse de $[0]_m$.
	}
	\xfield{
		$[0]_m$ n'a jamais d'inverse.
	}
\end{note}

\begin{note}
	\tags{}
	\xfield{
		Énumérer les propriétés d'un groupe commutatif $G$.
	}
	\xfield{
		\begin{itemize}
		\item \textbf{Associativité} $a \star (b \star c) = (a \star b) \star c$
		\item \textbf{Neutre} il existe un $e$ tel que $e \star a = a \star e = a$
		\item \textbf{Commutativité} $a \star b = b \star a$
		\item \textbf{Symétrique} $\forall a \in G \exists a' \Rightarrow a \star a' = e$
		\end{itemize}
	}
\end{note}

\begin{note}
	\tags{}
	\xfield{
		Comment calculer $K = m$ dans RSA?
	}
	\xfield{
		$$K = m = pq$$
	}
\end{note}

\begin{note}
\xfield{Comment calculer k dans RSA?}
\xfield{$$k = ppmc(p-1,q-1)$$}
\end{note}

\begin{note}
\xfield{Comment calculer le texte chiffré dans RSA}
\xfield{$$[C]_m, = [P^{e}]_m$$}
\end{note}

\begin{note}
\xfield{Comment calculer l'exposant de déchiffrement $f$ dans RSA?}
\xfield{$$[f]_k = [e]_{k}^{-1}$$}
\end{note}

\begin{note}
\xfield{Comment obtenir P à partir de C et f dans RSA?}
\xfield{$$P = [C^f]_m$$}
\end{note}

\begin{note}
\xfield{Donner la définition de la distance de Hamming.}
\xfield{La \textbf{Distance de Hamming} $:= d(x,y)$ est le nombre de positions où x et y diffèrent : \
$$d(x,y) := card\{i \in \{1...n\} : x_i \neq y_i\}$$
}
\end{note}

\begin{note}
\xfield{Donner la définition de \textbf{distance minimale} $d_{min}(C)$ pour un code correcteur C.}
\xfield{La distance minimale est la plus petite distance dans un ensemble de mots de code. Donné un ensemble C, on peut comparer toutes les distances de Hamming, et la plus petite est la distance minimale. Il faut cependant que les deux chaînes soient différentes.}
\end{note}

\begin{note}
\xfield{Dans quelles conditions un code correcteur peut-il faire une \textbf{détection d'erreur} de poids $\leq p$?}
\xfield{$$p < d_{min}(C)$$}
\end{note}

\begin{note}
\xfield{Dans quelles conditions un code correcteur peut-il faire une \textbf{correction d'effacement} de poids $\leq p$?}
\xfield{$$p < d_{min}(C)$$}
\end{note}

\begin{note}
\xfield{Dans quelles conditions un code correcteur peut-il faire une \textbf{correction d'erreur} de poids $\leq p$?}
\xfield{$$p < \frac{d_{min}(C)}{2}$$}
\end{note}

\begin{note}
\xfield{Donner les \textbf{deux} définitions de la \textbf{Borne de Singleton}. }
\xfield{
Pour un code en bloc C de longueur n, de rendement r et avec des longueurs de mot k.
$$d_{min}(C) \leq n(1-r) + 1$$
$$d_{min}(C) \leq n-k+1$$
}
\end{note}

\begin{note}
\xfield{Qu'est-ce que la caractéristique pour un corps fini?}
\xfield{La période de l'élément neutre de la multiplication. La caractéristique de ${\mathbb{Z}}/{p\mathbb{Z}}$ est p.}
\end{note}

\begin{note}
\xfield{Donner le Théorème en trois partie à la con des corps finis.}
\xfield{
\begin{itemize}
\item Le cardinal d'un corps fini est une puissance de sa caractéristique.
\item Tous les corps finis de même cardinal sont isomorphes.
\item Pour tout nombre premier $p$ et tout entier $m \geq 1$, il existe un corps fini de  cardinal $p^m$.
\end{itemize}
}
\end{note}

\begin{note}
\xfield{Que vaut le cardinal d'une corps fini $\varepsilon$ ?}
\xfield{Notons $n$ la charactéristique de $\varepsilon$, alors card($\varepsilon)=n^{\text{dim}(\varepsilon)}$}
\end{note}

\begin{note}
	\xfield{Notons $S$ l'ensemble des solutions dans $V=K^n$ de $m$ équation linéaire, un espace linéaire.\\
	Combien d'équation comporte la suite minimale d'équation linéaire dont l'ensemble des solutions est $S$ et dont les vecteurs de coefficients sont linéairement indépendants ?}
	\xfield{Avec dim($S$)=k, $n-k$ équations.}
\end{note}

\begin{note}
	\xfield{Quelle est la distance minimale d'un code de Reed Solomon de paramètre $(n,k)$?}
	\xfield{$\text{d}_\text{min} = n - k +1$, en d'autre terme, il atteint la borne de Singleton.}
\end{note}

\begin{note}
	\xfield{Comment créer un code de Reed Solomon, avec les paramètre $n,k$ donnés ?}
	\xfield{Toute d'abord explicitons la notation $P_{\vec{u}}(X)$ :\\
	Prenons $\vec{u}=(u_1,u_2, \hdots, u_k) \in F_k$ pour certains champs finis $F$. Chaque $\vec{u}$ définit un polynôme $P_{\vec{u}}(X)$ (dans
$F$) : \begin{align*}
&P_{\vec{u}}(X) = u_1 + u_2 x + u_3 x^2 + u_4 x^3 + \hdots + u_k x^{k-1}
\intertext{Par exemple, pour $\vec{u} = (3,2)$}
&P_{\vec{u}}(X) = 3 + 2X
\end{align*}
Maintenant pour créer un code de Reed Solomon avec les paramètre $1\le k \le n$:
\begin{enumerate}
\item L’alphabet est un corps fini $K$ de cardinal $\ge n$
\item Choisissons $n$ éléments distincts de $K$, $a_1 , a_2 , a_3 ,\hdots, a_n$ . Une suite de $k$ symboles $\vec{u} = (u_1 , u_2 , \hdots, u_k ) \in K^k$ est encodée en la suite de $n$ symboles $\vec{x} = (x_1 ,\hdots, x_n ) \in K^k$ définie par
\begin{align*}
x_i = P_{\vec{u}}(a_i) \text{ pour } i = 1,\hdots,n 
\end{align*}
\end{enumerate}
Le code de Reed Solomon $C$ est l’ensemble de tous les encodages possibles,
pour tous les $\vec{u} \in K^k$ . C’est donc un code en bloc de longueur $n$.}
\end{note}

\begin{note}
	\xfield{Comment construire facilement la matrice génératrice $G$ pour un code de type Reed Solomon ?}
	\xfield{En ayant déterminé les $n$ éléments $a_1,a_2,\hdots,a_n$ de l'alphabet du code, la matrice $G$ est :
	\begin{align*}
	G = \begin{pmatrix}
	1 & 1 & 1 & \cdots & 1\\
	a_1 & a_2 & a_3 & \cdots & a_n\\
	(a_1)^2 & (a_2)^2 & (a_3)^2 & \cdots & (a_n)^2\\
	\vdots & \vdots & \vdots & \ddots & \vdots\\
	(a_1)^{k-1} & (a_2)^{k-1} & (a_3)^{k-1} & \cdots & (a_n)^{k-1}
	\end{pmatrix}
	\end{align*} }
\end{note}

\end{document}
